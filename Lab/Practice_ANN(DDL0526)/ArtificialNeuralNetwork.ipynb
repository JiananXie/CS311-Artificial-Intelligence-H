{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "957594d9",
   "metadata": {},
   "source": [
    "# Practice 11: Artificial Neural Network\n",
    "In this lab, you need to implement a 4-layer artificial neural network and test your model on the MNIST dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2920bad6",
   "metadata": {},
   "source": [
    "### Part 1: Artificial Neural Network\n",
    "Artificial Neural Network (ANN) consists of multiple artificial neurons, usually having the structure of an input layer, multiple hidden layer, an output layer. **Layers** in the artificial neural network consist of **neurons**.\n",
    "\n",
    "Suppose that we have a N-layer ($N\\geq2$) ANN. The first layer is the input layer. The last layer is the output layer. For $n$-th layer ($n\\geq2$) with $d_n$ nodes, given the input from the last layer $x^{(n-1)}_i$ with $d_{n-1}$ nodes, the output of this layer is:\n",
    "$$x^{(n)}_i=g(a^{n})=g(w^{(n)}x^{(n-1)}+b^{(n)}),$$\n",
    "where $g$ is the activation function, $w^{n}$ is the weights from layer $n-1$ to layer $n$ with a size of $(d_n*d_{n-1})$, and $b^{(n)}$ is the bias.\n",
    "\n",
    "For example, in a 4-layer ANN, the output of each layer is processing in this way:\n",
    "$$\\mathbf{x} = \\mathbf{x}^{(1)} \\rightarrow \\mathbf{x}^{(2)} \\rightarrow \\mathbf{x}^{(3)}\\rightarrow \\mathbf{x}^{(4)} = \\hat{y}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Given a train dataset $\\mathbb{D}_{train}=\\{(\\mathbf{x}_i, y_i)|i \\in [1,m], \\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\{1, 2, ..., K\\}\\}$. The cross-entropy loss function is:\n",
    "$$Loss=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}-t_i^k log \\hat{y}^k_i + \\frac{\\lambda}{2}||W||^2_2,$$\n",
    "where $-t_i^k$ is the $k$-th value of the one-hot vector of $y_i$, $\\hat{y}^k_i$ is the $k$-th value of the output from the ANN, and $\\lambda$ is the weight of the regularization term.\n",
    "\n",
    "To minimize the loss function, we will conduct batch gradient descent algorithm. For the $n$-th layer ($n\\geq2$) in a $N$-layer ($N\\geq2$) ANN, the gradient of the loss function on weight matrix $w^{(n)}$ is:\n",
    "$$\\frac{\\partial Loss}{w^{(n)}} = \\frac{\\partial Loss}{\\partial x^{(N)}}\\frac{\\partial x^{(N)}}{\\partial x^{(N-1)}}...\\frac{\\partial x^{(n+1)}}{\\partial x^{(n)}}\\frac{\\partial x^{(n)}}{\\partial w^{(n)}}.$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this work, we will use relu activation function in hidden layers and the softmax function in the output layer.\n",
    "* The gradient on weigths of the output layer is: (derivation process can be found in <https://deepnotes.io/softmax-crossentropy>)\n",
    "$$\\frac{\\partial Loss}{w^{(N)}} = \\frac{\\partial Loss}{\\partial a^{(N)}}\\frac{\\partial (a^{N})}{\\partial w^{(N)}}= \\frac{1}{m}\\sum_{i=1}^{m}(x^{(N-1)}_i)^T*(\\hat{y}_i-y_i)$$\n",
    "let $\\delta_N=\\frac{\\partial Loss}{\\partial a^{(N)}}=(\\hat{y}_i-y_i)$, then \n",
    "$$\\frac{\\partial Loss}{w^{(N)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta_N(x^{(N-1)}_i)^T+\\lambda w^{(N)}$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(N)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_N$$\n",
    "\n",
    "* For the penultimate layer $N-1$ (derivation process can be found in <https://sudeepraja.github.io/Neural/>):\n",
    "$$\\frac{\\partial Loss}{w^{(N-1)}} = \\frac{\\partial Loss}{\\partial a^{(N-1)}}\\frac{\\partial a^{(N-1)}}{\\partial w^{(N-1)}} = \\frac{1}{m}\\sum_{i=1}^{m}(x^{(N-2)}_i)^T\\delta_{N-1},$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(N-1)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{N-1}$$\n",
    "where$$\\delta_{N-1}=\\frac{\\partial Loss}{\\partial a^{(N-1)}}=\\delta_{N}(w^{(N)})^T\\circ\\frac{\\partial g(a^{(N-1)})}{\\partial a^{(N-1)}}$$\n",
    "\n",
    "* So, for the $n$-th layer ($2\\leq n\\leq N-1$):\n",
    "$$\\frac{\\partial Loss}{w^{(n)}} = \\frac{\\partial Loss}{\\partial a^{(n)}}\\frac{\\partial a^{(n)}}{\\partial w^{(n)}} = \\frac{1}{m}\\sum_{i=1}^{m}(x^{(n-1)}_i)^T\\delta_{n}+ \\lambda w^{(n)},$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(n)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{n}$$\n",
    "where$$\\delta_{n}=\\frac{\\partial Loss}{\\partial a^{(n)}}=\\delta_{n+1}(w^{(n+1)})^T\\circ\\frac{\\partial g(a^{(n)})}{\\partial a^{(n)}}.$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Based on above derivations, you need to complete the missing code in the following."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2679afb9",
   "metadata": {},
   "source": [
    "### Part 2: Design a ANN for MNIST dataset\n",
    "\n",
    "MNIST stand for Mixed National Institute of Standards and Technology, whch has produced a handwritten digits dataset. This dataset is one of the most popular datasets in machine learning. \n",
    "\n",
    "The overall all dataset contains 60000 training images and 10000 testing images with 10 classes from 0 to 9, formmatted as $28*28$ pixel monochrome images.\n",
    "\n",
    "![MNIST](./MNIST.png)\n",
    "\n",
    "The mnist is already downloaded in the file `data.pkl`. Let's import the data and standarize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2414053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "(train_images, train_labels, \n",
    "     test_images, test_labels) = pickle.load(open('data.pkl', 'rb'),encoding='latin1')\n",
    "\n",
    "### normalize all pixels to [0,1) by dividing 256\n",
    "train_images = train_images/256.0\n",
    "test_images = test_images/256.0\n",
    "print(type(train_images[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98b6ea0b",
   "metadata": {},
   "source": [
    "Next, we need to design an ANN to conduct the classifcation task in MNIST dataset. As shown in PPT, this ANN have four layers:\n",
    "* Layer 1: input layer, size 784 = $28*28$\n",
    "* Layer 2: Hidden layer, size = 300, activation = relu\n",
    "* Layer 3: Hidden layer, size = 100, activation = relu\n",
    "* Layer 4: Output layer , size = 10, activation = softmax\n",
    "![ANN](./ANN.png)\n",
    "You need to complete the following parts:\n",
    "1. Initialize the all parameters in ANN\n",
    "2. Complete the forward process of hidden layer\n",
    "3. Complete the forward process of output layer\n",
    "4. Complete the loss calculation function\n",
    "5. Calculate the gradient on these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3790a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def xavier(n1, n2):\n",
    "    \"\"\"\n",
    "    Initialize a matrix with a shape of (n1, n2)\n",
    "    \n",
    "    :param n1: int, number of rows of the matrix\n",
    "    :param n2: int, number of columns of the matrix\n",
    "    \n",
    "    :return:\n",
    "        ndarray: numpy.ndarray, a random matrix generated by the uniform distribution.\n",
    "    \"\"\"\n",
    "    x = np.sqrt(6*1.0/(n1 + n2))\n",
    "    ndarray = np.random.uniform(low=-x, high=x, size=(n1, n2))\n",
    "    \n",
    "    return ndarray\n",
    "\n",
    "def softmax(ndarray):\n",
    "    \"\"\"\n",
    "    Softmax function for input with a shape of (batch_size, n_classes)\n",
    "    \n",
    "    :param ndarray: numpy.ndarray, the output of the last layer in ANN with a shape of (batch_size, n_classes)\n",
    "    \n",
    "    :return:\n",
    "        numpy.ndarray, the softmax output with a shape of (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    r = ndarray.shape[0]\n",
    "    c = ndarray.shape[1]\n",
    "    max_arr = np.tile(ndarray.max(axis=1).reshape(r,1), (1,c))\n",
    "    e_x = np.exp(ndarray - max_arr)\n",
    "    return e_x / np.tile(e_x.sum(axis=1).reshape(r,1), (1,c))\n",
    "\n",
    "def relu(ndarray):\n",
    "    return np.maximum(ndarray, 0)\n",
    "\n",
    "def d_relu(ndarray):\n",
    "    ndarray[ndarray>0] = 1\n",
    "    ndarray[ndarray<=0] = 0\n",
    "    return ndarray\n",
    "\n",
    "def getLabel(ndarray,BATCHSIZE,num_output):\n",
    "    label = np.zeros((BATCHSIZE, num_output))\n",
    "    for i in range(0, BATCHSIZE):\n",
    "        idx = ndarray[i]\n",
    "        label[i][idx] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parameter_initialization(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    \"\"\"\n",
    "    Parameter initialization function for our ANN.\n",
    "    \n",
    "    :param input_size: int\n",
    "    :param hidden1_size: int\n",
    "    :param hidden2_size: int\n",
    "    :param output_size: int\n",
    "    \n",
    "    :return:\n",
    "        Parameter: dict, a dictionary to store the parameters in ANN.\n",
    "    \"\"\"\n",
    "    # TODO 1:\n",
    "    # Please Initialize all parameters used in ANN-Hidden Layers with Xavier\n",
    "    Parameter={}\n",
    "    #Your code starts here\n",
    "    w2 = xavier(input_size, hidden1_size)\n",
    "    b2 = xavier(1,hidden1_size)\n",
    "\n",
    "    w3 = xavier(hidden1_size, hidden2_size)\n",
    "    b3 = xavier(1, hidden2_size)\n",
    "\n",
    "    w4 = xavier(hidden2_size, output_size)\n",
    "    b4 = xavier(1, output_size)\n",
    "    #Your code ends here\n",
    "    \n",
    "    Parameter['w2']=w2\n",
    "    Parameter['b2']=b2\n",
    "    \n",
    "    Parameter['w3']=w3\n",
    "    Parameter['b3']=b3\n",
    "    \n",
    "    Parameter['w4']=w4\n",
    "    Parameter['b4']=b4\n",
    "    \n",
    "    return Parameter\n",
    "\n",
    "def Hidden_Layer(x, w, b):\n",
    "    \"\"\"\n",
    "    Return the output of the hidden layer.\n",
    "\n",
    "    :param x: numpy.ndarray with a shape of (n, input_dim), input data of this layer.\n",
    "    :param w: numpy.ndarray, weights of this layer.\n",
    "    :param b: numpy.ndarray, bias of this layer.\n",
    "    \n",
    "    :return:\n",
    "        z: numpy.ndarray with a shape of (n, output_dim), output data of this layer\n",
    "    \"\"\"        \n",
    "    # TODO 2: generate the outputs of this layer\n",
    "    # Your code starts here\n",
    "    z = relu(x @ w + b)\n",
    "    # Your code ends here\n",
    "\n",
    "    return z\n",
    "\n",
    "def Output_Layer(x, w, b):\n",
    "    \"\"\"\n",
    "    Return the output of the output layer.\n",
    "\n",
    "    :param x: numpy.ndarray with a shape of (n, input_dim), input data of this layer.\n",
    "    :param w: numpy.ndarray, weights of this layer.\n",
    "    :param b: numpy.ndarray, bias of this layer.\n",
    "    \n",
    "    :return:\n",
    "        z: numpy.ndarray with a shape of (n, num_classes), output of the output layer\n",
    "    \"\"\"        \n",
    "    # TODO 3: generate the outputs of this layer\n",
    "    # Your code starts here\n",
    "    z = softmax(x @ w +b)\n",
    "    # Your code ends here\n",
    "\n",
    "    return z\n",
    "\n",
    "def Loss(label, output):\n",
    "    \"\"\"\n",
    "    Return the loss of ANN.\n",
    "\n",
    "    :param label: numpy.ndarray with a shape of (n, num_classes), true labels (each line is a one-hot vector)\n",
    "    :param output: numpy.ndarray with a shape of (n, num_classes), predicted results of your model\n",
    "    \n",
    "    :return:\n",
    "        cross_entropy_loss: float, \n",
    "    \"\"\"\n",
    "    # TODO 4: calculate the loss of the output\n",
    "    # Your code starts here\n",
    "    n = label.shape[0]\n",
    "    cross_entropy_loss = - np.sum(np.diag(np.log(output) @ label.T)) / n\n",
    "    # Your code ends here\n",
    "    \n",
    "    return cross_entropy_loss\n",
    "\n",
    "def BP_output(predict, label, w, x , lam):\n",
    "    \"\"\"\n",
    "    Return the gradients of loss on the weight of output layer, and the delta_N\n",
    "    \n",
    "    :param predict: numpy.ndarray with a shape of (n, num_classes), predicted results of your model\n",
    "    :param label: numpy.ndarray with a shape of (n, num_classes), true labels (each line is a one-hot vector)\n",
    "    :param w: weight matrix from hidden layer 2 to output layer\n",
    "    :param x: input of the output layer\n",
    "    :param lam: the weight of the regularization term\n",
    "    \n",
    "    :return:\n",
    "        grad_w: gradient of loss on the weight of output layer\n",
    "        grad_b: gradient of loss on the bias of output layer\n",
    "        delta_N: partial loss / partial a_N\n",
    "    \"\"\"\n",
    "    # TODO 5: calculate the gradients of loss on the weight of output layer\n",
    "    n = predict.shape[0]\n",
    "    # croos_entropy_loss_regulation = Loss(label, predict) + lam * np.sum(np.square(w)) / 2\n",
    "    delta_N = predict - label\n",
    "    grad_w = (x.T @ delta_N) / n + lam * w\n",
    "    grad_b = np.sum(delta_N,axis=0) / n\n",
    "    \n",
    "    return  grad_w, grad_b, delta_N\n",
    "\n",
    "def BP_hidden(delta_n_1, w_n_1, w_n, b_n, x, lam):\n",
    "    \"\"\"\n",
    "    Return the gradients of loss on the weight of layer n and the delta_n.\n",
    "    \n",
    "    :param delta_n_1: numpy.ndarray with a shape of (n, size_layer(n+1)), delta of the layer n+1\n",
    "    :param w_n_1: weight matrix of layer n+1 with a shape of (size_layer(n), size_layer(n+1))\n",
    "    :param w_n: weight matrix of layer n with a shape of (size_layer(n-1), size_layer(n))\n",
    "    :param b_n: bias of layer n with a shape of (1, size_layer(n))\n",
    "    :param x: input of layer n with a shape of (n, size_layer(n-1))\n",
    "    :param lam: the weight of the regularization term\n",
    "    \n",
    "    :return:\n",
    "        grad_w: gradient of loss on the weight of current hidden layer n\n",
    "        grad_b: gradient of loss on the bias of current hidden layer n\n",
    "        delta_n: partial loss / partial a_n\n",
    "    \"\"\"\n",
    "    # TODO 6: calculate the gradients of loss on the weight of output layer  \n",
    "    n = delta_n_1.shape[0]\n",
    "    a = d_relu(x @ w_n + b_n)\n",
    "    delta_n = (delta_n_1 @ w_n_1.T) * a \n",
    "    grad_w = (x.T @ delta_n) / n  + lam * w_n\n",
    "    grad_b = np.sum(delta_n,axis=0) / n\n",
    "    return  grad_w, grad_b, delta_n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50417942",
   "metadata": {},
   "source": [
    "After finishing the ANN codes, let's test it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c95ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hyper parameters\n",
    "EPOCH = 100\n",
    "ITERS = 100\n",
    "BATCHSIZE = 100\n",
    "LR_BASE = 0.1\n",
    "lam = 0.0005  # lambda\n",
    "num_input = 784\n",
    "num_layer1 = 300\n",
    "num_layer2 = 100\n",
    "num_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe7a90b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "0.2706318099093056\n",
      "Accuracy: \n",
      "0.862\n",
      "Epoch 2: \n",
      "0.19774663855467342\n",
      "Accuracy: \n",
      "0.887\n",
      "Epoch 3: \n",
      "0.1688698948347658\n",
      "Accuracy: \n",
      "0.908\n",
      "Epoch 4: \n",
      "0.14450338308892596\n",
      "Accuracy: \n",
      "0.915\n",
      "Epoch 5: \n",
      "0.12567268217503316\n",
      "Accuracy: \n",
      "0.918\n",
      "Epoch 6: \n",
      "0.10982257902679775\n",
      "Accuracy: \n",
      "0.924\n",
      "Epoch 7: \n",
      "0.09619273041607597\n",
      "Accuracy: \n",
      "0.926\n",
      "Epoch 8: \n",
      "0.08485033214871254\n",
      "Accuracy: \n",
      "0.928\n",
      "Epoch 9: \n",
      "0.07447335691572948\n",
      "Accuracy: \n",
      "0.931\n",
      "Epoch 10: \n",
      "0.06634529433303303\n",
      "Accuracy: \n",
      "0.9339999999999999\n",
      "Epoch 11: \n",
      "0.059831192750587944\n",
      "Accuracy: \n",
      "0.9359999999999999\n",
      "Epoch 12: \n",
      "0.05396215188333953\n",
      "Accuracy: \n",
      "0.9359999999999999\n",
      "Epoch 13: \n",
      "0.048483002026992156\n",
      "Accuracy: \n",
      "0.938\n",
      "Epoch 14: \n",
      "0.044058464832098077\n",
      "Accuracy: \n",
      "0.938\n",
      "Epoch 15: \n",
      "0.040211647307487317\n",
      "Accuracy: \n",
      "0.94\n",
      "Epoch 16: \n",
      "0.03717951647606219\n",
      "Accuracy: \n",
      "0.9410000000000001\n",
      "Epoch 17: \n",
      "0.034215003707147716\n",
      "Accuracy: \n",
      "0.943\n",
      "Epoch 18: \n",
      "0.031452155087238025\n",
      "Accuracy: \n",
      "0.943\n",
      "Epoch 19: \n",
      "0.029052380544776022\n",
      "Accuracy: \n",
      "0.945\n",
      "Epoch 20: \n",
      "0.026741515226852686\n",
      "Accuracy: \n",
      "0.944\n",
      "Epoch 21: \n",
      "0.02501909943135113\n",
      "Accuracy: \n",
      "0.945\n",
      "Epoch 22: \n",
      "0.023211765471368117\n",
      "Accuracy: \n",
      "0.946\n",
      "Epoch 23: \n",
      "0.021758005447482614\n",
      "Accuracy: \n",
      "0.946\n",
      "Epoch 24: \n",
      "0.020226018573952634\n",
      "Accuracy: \n",
      "0.946\n",
      "Epoch 25: \n",
      "0.01886185373158863\n",
      "Accuracy: \n",
      "0.947\n",
      "Epoch 26: \n",
      "0.01774107322210277\n",
      "Accuracy: \n",
      "0.947\n",
      "Epoch 27: \n",
      "0.01662305227093074\n",
      "Accuracy: \n",
      "0.948\n",
      "Epoch 28: \n",
      "0.01567431468111336\n",
      "Accuracy: \n",
      "0.95\n",
      "Epoch 29: \n",
      "0.014810980047351634\n",
      "Accuracy: \n",
      "0.95\n",
      "Epoch 30: \n",
      "0.014087294491956061\n",
      "Accuracy: \n",
      "0.951\n",
      "Epoch 31: \n",
      "0.013409310748641863\n",
      "Accuracy: \n",
      "0.951\n",
      "Epoch 32: \n",
      "0.012870041806912756\n",
      "Accuracy: \n",
      "0.951\n",
      "Epoch 33: \n",
      "0.012258568163160606\n",
      "Accuracy: \n",
      "0.95\n",
      "Epoch 34: \n",
      "0.011746288985157071\n",
      "Accuracy: \n",
      "0.951\n",
      "Epoch 35: \n",
      "0.011369074866446591\n",
      "Accuracy: \n",
      "0.952\n",
      "Epoch 36: \n",
      "0.010918390291280961\n",
      "Accuracy: \n",
      "0.952\n",
      "Epoch 37: \n",
      "0.010474828749951308\n",
      "Accuracy: \n",
      "0.952\n",
      "Epoch 38: \n",
      "0.010069642827190474\n",
      "Accuracy: \n",
      "0.953\n",
      "Epoch 39: \n",
      "0.009776829908462969\n",
      "Accuracy: \n",
      "0.953\n",
      "Epoch 40: \n",
      "0.009438380463408596\n",
      "Accuracy: \n",
      "0.953\n",
      "Epoch 41: \n",
      "0.009221491765370774\n",
      "Accuracy: \n",
      "0.953\n",
      "Epoch 42: \n",
      "0.008924740196946586\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 43: \n",
      "0.008664484432767216\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 44: \n",
      "0.008440157929364818\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 45: \n",
      "0.00823857966950119\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 46: \n",
      "0.00805251073248291\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 47: \n",
      "0.00787209306202352\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 48: \n",
      "0.007663779602206358\n",
      "Accuracy: \n",
      "0.957\n",
      "Epoch 49: \n",
      "0.007583497114061108\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 50: \n",
      "0.007423734831592096\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 51: \n",
      "0.007272302637628797\n",
      "Accuracy: \n",
      "0.957\n",
      "Epoch 52: \n",
      "0.007133464511606608\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 53: \n",
      "0.007261635142431713\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 54: \n",
      "0.007320428365676704\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 55: \n",
      "0.007360103928464373\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 56: \n",
      "0.007386289677203959\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 57: \n",
      "0.0074088898403657085\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 58: \n",
      "0.007423524365038676\n",
      "Accuracy: \n",
      "0.954\n",
      "Epoch 59: \n",
      "0.0074343560564363205\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 60: \n",
      "0.007442386642348622\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 61: \n",
      "0.007446874724519243\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 62: \n",
      "0.0074492872136180215\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 63: \n",
      "0.007446182515790894\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 64: \n",
      "0.007446318680356323\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 65: \n",
      "0.007437444165775895\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 66: \n",
      "0.00743516436484451\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 67: \n",
      "0.007427230195123103\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 68: \n",
      "0.007419639935061981\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 69: \n",
      "0.007412377094542707\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 70: \n",
      "0.007402584246468411\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 71: \n",
      "0.007390547864198916\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 72: \n",
      "0.00737842503228557\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 73: \n",
      "0.007367543682549156\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 74: \n",
      "0.00735607432665957\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 75: \n",
      "0.0073418242325270075\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 76: \n",
      "0.007332719620432169\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 77: \n",
      "0.007319839494409838\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 78: \n",
      "0.00730482241896107\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 79: \n",
      "0.0072932550022524615\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 80: \n",
      "0.007280576476019387\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 81: \n",
      "0.007269848796328288\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 82: \n",
      "0.007252739156908999\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 83: \n",
      "0.007243431546736739\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 84: \n",
      "0.007226302970030184\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 85: \n",
      "0.007213196345917251\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 86: \n",
      "0.0072020012353428845\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 87: \n",
      "0.007188544355006677\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 88: \n",
      "0.007175735486507121\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 89: \n",
      "0.0071630717309435965\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 90: \n",
      "0.007149817647797423\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 91: \n",
      "0.007139771219892874\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 92: \n",
      "0.007123299861391677\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 93: \n",
      "0.007118566094714011\n",
      "Accuracy: \n",
      "0.956\n",
      "Epoch 94: \n",
      "0.007098390686087719\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 95: \n",
      "0.007091955083405054\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 96: \n",
      "0.007078502307107006\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 97: \n",
      "0.007064421744019876\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 98: \n",
      "0.007058177620115128\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 99: \n",
      "0.007043330288134074\n",
      "Accuracy: \n",
      "0.955\n",
      "Epoch 100: \n",
      "0.007034088255785324\n",
      "Accuracy: \n",
      "0.955\n"
     ]
    }
   ],
   "source": [
    "# 2. Weight initialization: Xavier\n",
    "Parameter = Parameter_initialization(num_input,num_layer1,num_layer2,num_output)\n",
    "w2, w3, w4=Parameter['w2'], Parameter['w3'], Parameter['w4']\n",
    "b2, b3, b4=Parameter['b2'], Parameter['b3'], Parameter['b4']\n",
    "\n",
    "# 3. training of neural network\n",
    "loss = np.zeros((EPOCH))   #save the loss of each epoch\n",
    "accuracy = np.zeros((EPOCH))  #save the accuracy of each epoch\n",
    "for epoch in range(0, EPOCH):\n",
    "    if epoch <= EPOCH/2:\n",
    "        lr = LR_BASE\n",
    "    else:\n",
    "        lr = LR_BASE / 10.0\n",
    "    for iters in range(0, ITERS):\n",
    "        image_blob = train_images[iters*BATCHSIZE:(iters+1)*BATCHSIZE, :] # 100*784\n",
    "        label_blob = train_labels[iters*BATCHSIZE:(iters+1)*BATCHSIZE] # 100*1\n",
    "        label = getLabel(label_blob,BATCHSIZE,num_output)\n",
    "        # Forward propagation  Hidden Layer\n",
    "        x1 = image_blob\n",
    "        x2 = Hidden_Layer(x1,w2,b2)\n",
    "        x3 = Hidden_Layer(x2,w3,b3)\n",
    "        # Forward propagation  output Layer\n",
    "        x4 = Output_Layer(x3,w4,b4)\n",
    "        y_hat = x4\n",
    "        if np.count_nonzero(y_hat) != 1000:\n",
    "            print(\"y_hat\",y_hat)\n",
    "        #comupte loss\n",
    "        loss_tmp = Loss(label,y_hat)\n",
    "\n",
    "        if iters % 100 == 99:\n",
    "            loss[epoch] = loss_tmp\n",
    "            print('Epoch '+str(epoch+1)+': ')\n",
    "            print(loss_tmp)\n",
    "        # Back propagation\n",
    "        grad_w4, grad_b4, delta_4 = BP_output(y_hat, label, w4, x3, lam) # output layer\n",
    "        grad_w3, grad_b3, delta_3 = BP_hidden(delta_4, w4, w3, b3, x2, lam) # hidden layer 2\n",
    "        grad_w2, grad_b2, delta_2 = BP_hidden(delta_3, w3, w2, b2, x1, lam) # hidden layer 1\n",
    "\n",
    "        # Gradient update\n",
    "        w2 = w2 - lr*(grad_w2)\n",
    "        w3 = w3 - lr*(grad_w3)\n",
    "        w4 = w4 - lr*(grad_w4)\n",
    "        b2 = b2 - lr*(grad_b2)\n",
    "        b3 = b3 - lr*(grad_b3)\n",
    "        b4 = b4 - lr*(grad_b4)\n",
    "        # Testing for accuracy\n",
    "        if iters % 100 == 99:\n",
    "            x1 = test_images\n",
    "            x2 = Hidden_Layer(x1,w2,b2)\n",
    "            x3 = Hidden_Layer(x2,w3,b3)\n",
    "            # Forward propagation  output Layer\n",
    "            x4 = Output_Layer(x3,w4,b4)\n",
    "            y_hat = x4\n",
    "            \n",
    "            predict = np.argmax(y_hat, axis=1)\n",
    "            print('Accuracy: ')\n",
    "            accuracy[epoch] = 1 - np.count_nonzero(predict - test_labels)*1.0/1000\n",
    "            print(accuracy[epoch])\n",
    "\n",
    "### 4. Plot\n",
    "plt.figure(figsize=(12,5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(np.arange(EPOCH)+1, loss[0:], 'r', label='Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss on trainSet', fontsize=16)\n",
    "plt.grid()\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(np.arange(EPOCH)+1, accuracy[0:], 'b', label='Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy on trainSet', fontsize=16)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
